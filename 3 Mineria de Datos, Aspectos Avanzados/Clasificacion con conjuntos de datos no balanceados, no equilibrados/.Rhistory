rgl.open()
par3d(windowRect = 50 + c( 0, 0, width, width ) )
rgl.bg(color = bg )
}
rgl.clear(type = c("shapes", "bboxdeco"))
rgl.viewpoint(theta = 15, phi = 20, zoom = 0.7)
}
# x, y, z : numeric vectors corresponding to
#  the coordinates of points
# axis.col : axis colors
# xlab, ylab, zlab: axis labels
# show.plane : add axis planes
# show.bbox : add the bounding box decoration
# bbox.col: the bounding box colors. The first color is the
# the background color; the second color is the color of tick marks
rgl_add_axes <- function(x, y, z, axis.col = "grey",
xlab = "", ylab="", zlab="", show.plane = TRUE,
show.bbox = FALSE, bbox.col = c("#333377","black"))
{
lim <- function(x){c(-max(abs(x)), max(abs(x))) * 1.1}
# Add axes
xlim <- lim(x); ylim <- lim(y); zlim <- lim(z)
rgl.lines(xlim, c(0, 0), c(0, 0), color = axis.col)
rgl.lines(c(0, 0), ylim, c(0, 0), color = axis.col)
rgl.lines(c(0, 0), c(0, 0), zlim, color = axis.col)
# Add a point at the end of each axes to specify the direction
axes <- rbind(c(xlim[2], 0, 0), c(0, ylim[2], 0),
c(0, 0, zlim[2]))
rgl.points(axes, color = axis.col, size = 3)
# Add axis labels
rgl.texts(axes, text = c(xlab, ylab, zlab), color = axis.col,
adj = c(0.5, -0.8), size = 2)
# Add plane
if(show.plane)
xlim <- xlim/1.1; zlim <- zlim /1.1
rgl.quads( x = rep(xlim, each = 2), y = c(0, 0, 0, 0),
z = c(zlim[1], zlim[2], zlim[2], zlim[1]))
# Add bounding box decoration
if(show.bbox){
rgl.bbox(color=c(bbox.col[1],bbox.col[2]), alpha = 0.5,
emission=bbox.col[1], specular=bbox.col[1], shininess=5,
xlen = 3, ylen = 3, zlen = 3)
}
}
# Ejemplo que ilustra el modelo de clasificación usando una variable
datos <-data.frame(y=as.numeric(as.numeric(iris$Species)),
x4=iris$Sepal.Width,
x1=iris$Sepal.Length,
x2=iris$Petal.Length,
x3=iris$Petal.Width)
library(splines)
model3 <- glm(y~ns(x2,16), data =datos)
b <- predict(model3, datos)
rb <- round(b)
x <- seq(1,7,0.01)
x <- rbind(x2=x,y=0)
x <-t(x)
x <-as.data.frame(x)
xy <- predict(model3, newdata = x)
rxy <-round(xy)
plot(datos$x2, datos$y, col = datos$y, xlab = "Longitud del Sépalo", ylab = "Tipo de Iris", title("Specie ~ ns(Petal.Length,16)"))
lines(x[,1],xy,col=ifelse(xy<=1.5, "black", ifelse(xy<=2.5, "red", "green")))
segments(datos$x2,datos$y,datos$x2,b, col=datos$y, lty=4)
abline(2.5,0,col="green")
abline(1.5,0,col="red")
# Ejemplo que ilustra el modelo de clasificación en base a 2 variables
model3 <- glm(y~ns(x2,16)+ns(x1,16), data =datos)
c <- predict(model3, newdata = datos)
plot(datos$x2,datos$x1,col=round(c),pch=0,
xlab = "Longitud del Sépalo", ylab = "Longitud del Pétalo", title("Specie ~ ns(Petal.Length,16)+ns(Sepal.Length,16)"))
points(datos$x2,datos$x1,col=datos$y,pch="x")
#### Ilustrar como se distribuye los espacios de clasificación
plantilla <-matrix(c(0,0,0),ncol=3)
for (p1 in seq(1,8,0.05))
for (p2 in seq(1,7,0.05))
plantilla <-rbind(plantilla, c(x1=p1,x2=p2,y=0))
plantilla <- as.data.frame(plantilla)
plantilla[,3] <- predict(model3, newdata = plantilla)
plot(datos$x2,datos$x1,col=round(c),pch=0,
xlab = "Longitud del Sépalo", ylab = "Longitud del Pétalo", title("Specie ~ ns(Petal.Length,16)+ns(Sepal.Length,16)"))
points(plantilla$x2,plantilla$x1,
col= ifelse(round(plantilla$y)<1, 0, ifelse(round(plantilla$y)>3, 0, round(plantilla$y) )),pch=20)
points(datos$x2,datos$x1,col=round(c),pch=0)
points(datos$x2,datos$x1,col=datos$y,pch="x")
# Ejemplo que ilustra el modelo de clasificación en base a 3 variables
model3 <- glm(y~ns(x2,16)+ns(x1,16)+ns(x3,16), data =datos)
c <- predict(model3, newdata = datos)
plantilla <-matrix(c(0,0,0,0),ncol=4)
for (p1 in seq(1,8,(8-1)/10))
for (p2 in seq(1,7,(7-1)/10))
for (p3 in seq(0, 2.5, 2.5/10 ))
plantilla <-rbind(plantilla, c(x1=p1,x2=p2,x3=p3,y=0))
plantilla <- as.data.frame(plantilla)
plantilla[,4] <- round(predict(model3, newdata = plantilla))
plantilla[,4] <- ifelse(plantilla[,4]<1,"white", ifelse(plantilla[,4]>3,"white",plantilla[,4]))
#saca la grafica en 3d
library(rgl)
rgl_init()
rgl.spheres(x=datos$x2, y=datos$x1, z=datos$x3, r=0.1, col=datos$y)
rgl_add_axes(x=datos$x2, y=datos$x1, z=datos$x3, show.bbox = T)
aspect3d(1,1,1)
#### Ilustra como se distribuyen los espacios de clasificación
rgl_init()
rgl.spheres(x=plantilla[,1], y=plantilla[,2], z=plantilla[,3], r=0.1, col=plantilla[,4])
#rgl_add_axes(x=plantilla[,1], y=plantilla[,2], z=plantilla[,3], show.bbox = T)
aspect3d(1,1,1)
plot3d(x=plantilla[,1], y=plantilla[,2], z=plantilla[,3], col=plantilla[,4])
rgl_init()
rgl.spheres(x=plantilla[,1], y=plantilla[,2], z=plantilla[,3], r=0.1, col=plantilla[,4])
rgl_add_axes(x=plantilla[,1], y=plantilla[,2], z=plantilla[,3], show.bbox = T)
aspect3d(1,1,1)
plot3d(x=plantilla[,1], y=plantilla[,2], z=plantilla[,3], col=plantilla[,4])
library(tree)
summary(iris)
# Construir un arbol que clasifica la especie en base al resto de variables
tree.iris = tree(Species~.,iris)
library(tree)
install.packages("tree")
summary(iris)
# Construir un arbol que clasifica la especie en base al resto de variables
tree.iris = tree(Species~.,iris)
library(tree)
summary(iris)
# Construir un arbol que clasifica la especie en base al resto de variables
tree.iris = tree(Species~.,iris)
summary(tree.iris)
plot(tree.iris)
text(tree.iris, pretty=0)
tree.iris
# Dividir en training y test
set.seed (2)
train=sample (1:nrow(iris), 100)
iris.test=iris [-train ,]
# Construyo el arbol sobre el conjunto de entrenamiento
tree.iris =tree(Species~. ,iris ,subset =train )
# Aplico el arbol sobre el conjunto de test
tree.pred =predict (tree.iris ,iris.test ,type ="class")
# Visualizo la matriz de confusion
table(tree.pred , iris.test[,5])
# Podar el arbol usando cv
set.seed (3)
cv.iris = cv.tree(tree.iris ,FUN=prune.misclass )
names(cv.iris )
cv.iris
# Pintamos el error
par(mfrow =c(1,2))
plot(cv.iris$size ,cv.iris$dev ,type="b")
plot(cv.iris$k ,cv.iris$dev ,type="b")
# Ahora podamos el arbol con prune.misclass
prune.iris =prune.misclass (tree.iris ,best =3)
par(mfrow =c(1,1))
plot(prune.iris)
text(prune.iris ,pretty =0)
# Como se comportara este arbol en su capacidad de prediccion
tree.pred=predict (prune.iris , iris.test ,type="class")
table(tree.pred ,iris.test[,5])
# Ahora podemos modificar el tamanio del arbol modificando best
prune.iris =prune.misclass (tree.iris ,best =4)
plot(prune.iris)
text(prune.iris ,pretty =0)
tree.pred=predict (prune.iris , iris.test ,type="class")
table(tree.pred ,iris.test[,5])
# Random Forest
library (randomForest)
set.seed (1)
bag.iris = randomForest(Species~., data=iris, subset=train)
bag.iris
yhat.bag = predict (bag.iris ,newdata =iris.test)
yhat.bag
# Construyo una funcion para calcular el acierto a partir del RandomForest
acierto <- function(bag.datos){
return (sum (sapply(1:length(bag.datos$y), function(x){
if (is.na(bag.datos$predicted[x])){
0
}
else if (as.numeric(bag.datos$y[x])==as.numeric(bag.datos$predicted[x])){
1
}
else{
0
}
}))/length(bag.datos$y))
}
resul = as.data.frame(cbind(predicted = yhat.bag, y=iris.test[,5]))
acierto(resul)
# Fijando el numero de arboles
bag.iris = randomForest(Species~.,data=iris ,subset =train , ntree=25)
bag.iris
acierto(bag.iris)
bag.func = randomForest(formula,data=datos,ntree=num_trees)
bag.func = randomForest(formula,data=datos,subset=train,ntree=num_trees)
Graphical_RF <- function(datos, formula, num_trees){
# Definir aqui la funcion
bag.func = randomForest(formula,data=datos,subset=train,ntree=num_trees)
acierto(bag.func)
}
Graphical_RF(iris,Species~.,100)
Graphical_RF <- function(datos, formula, num_trees){
# Definir aqui la funcion
sapply(1:num_trees, function(x){
bag.func = randomForest(formula,data=datos,subset=train,ntree=x)
acierto(bag.func)
})
}
Graphical_RF(iris,Species~.,100)
Graphical_RF <- function(datos, formula, num_trees){
# Definir aqui la funcion
vec <- sapply(1:num_trees, function(x){
bag.func = randomForest(formula,data=datos,subset=train,ntree=x)
acierto(bag.func)
})
}
Graphical_RF(iris,Species~.,100)
vec
Graphical_RF <- function(datos, formula, num_trees){
# Definir aqui la funcion
vec <- sapply(1:num_trees, function(x){
bag.func = randomForest(formula,data=datos,subset=train,ntree=x)
acierto(bag.func)
})
vec
}
Graphical_RF(iris,Species~.,100)
Graphical_RF <- function(datos, formula, num_trees){
# Definir aqui la funcion
vec <- sapply(1:num_trees, function(x){
bag.func = randomForest(formula,data=datos,subset=train,ntree=x)
acierto(bag.func)
})
plot(1:num_trees,vec)
}
Graphical_RF(iris,Species~.,100)
lines()
lines(vec)
Graphical_RF <- function(datos, formula, num_trees){
# Definir aqui la funcion
vec <- sapply(1:num_trees, function(x){
bag.func = randomForest(formula,data=datos,subset=train,ntree=x)
acierto(bag.func)
})
plot(1:num_trees,vec)
lines(vec)
}
Graphical_RF(iris,Species~.,100)
summary(iris)
summary(Auto)
Auto
Autos
summary(Autos)
library(ISLR)
summary(Autos)
summary(Auto)
dim(Auto)
dim(iris)
set.seed (2)
train=sample (1:nrow(Auto), 300)
datos = Auto
Graphical_RF(datos,origin~.-name,100)
train=sample (1:nrow(iris), 100)
set.seed (2)
train=sample (1:nrow(iris), 100)
#FRANCISCO PÉREZ HERNÁNDEZ 20076629K
####### INTRODUCCIÓN A R ######
# EJERCICIO 1: R Interactivo
#1.1 Crea una secuencia de números impares
e11a = seq(1,30, by =2)
#1.2 Crea números del 1 al 30
e12a = c(1:30)
#1.3 Busca en la ayuda que hace la función seq(). Describe que hace. Utilízala para crear números del 1 al 30 con un
#incremento de 0.5. ¿Qué otros parámetros te ofrece la función seq()? Utilízalos en un ejemplo.
#Para saber que hace seq:
help("seq")
#Genera secuencias regulares
e13a = seq(1,30, by = 0.5)
#Sus parametros son, from,to,by,length.out,along.with,...
e13b= seq(1,30,length.out=3)
e13c =seq_along(along.with = c(323,2))
#1.4 Crea una secuencia de números indicando el principio y la longitud de la secuencia de números
e14a = seq(1, length.out = 4)
#1.5 Crea letras minúsculas, mayúsculas, nombre de los meses del año y nombre de los meses del año abreviado
e15a = letters
e15b = LETTERS
e15c = month.name
e15d = month.abb
#1.6 Investiga la función rep(). Repite un vector del 1 al 8 cinco veces.
help(rep)
e16a = rep(c(1:8),5)
#1.7 Haz lo mismo con las primeras ocho letras del abecedario en mayúsculas
e17a = rep(LETTERS[1:8],5)
# EJERCICIO 2: Vectores
#2.1 Crea los siguientes vectores:
#2.1a un vector del 1 al 20:
e21a = c(1:20)
#2.1b un vector del 20 al 1:
e21b = c(20:1)
#2.1c Utilizando el comando c() crea un vector que tenga el siguiente patrón 1,2,3,4,5…20,19,18,17….1
e21c = c(e21a,e21b[2:length(e21b)])
#2.2 Genera una secuencia de números del 1 al 30 utilizando el operador : y asígnalo al vector x. El vector resultante
#x tiene 30 elementos. Recuerda que el operador ‘:’ tiene prioridad sobre otros operadores aritméticos en una expresión.
e22 = c(1:30)
#2.3 Genera un vector x que contenga números del 1 al 9. Utilizando el operador ‘:’ .  y utilizando otras opciones.
#PISTA: seq()
e23a = c(1:9)
e23b = seq(1:9)
#2.4 Genera un vector x que contenga 9 números comprendidos entre 1 y 5
e24 = seq(1,5,length.out = 9)
#2.5 Busca que hace la función sequence(). ¿Cual es la diferencia con la función seq()
help(sequence)
help(seq)
sequence(c(3,2,4))
seq(c(3,2,4))
#Con sequence se crean secuencias hasta los valores indicados en el vector mientras que con seq se crearia
#un vector con tantos elementos como haya en un vector
#2.6 Crea un vector numérico utilizando la función c()
e26 = as.numeric(c(2:45))
#2.7 Accede al segundo elemento del vector
e26[2]
#2.8 Crea un vector numérico “z” que contenga del 1 al 10. Cambia el modo del vector a carácter.
e28a = as.numeric(c(1:10))
e28b = as.character(e28a)
#2.9 Ahora cambia el vector z a numérico de nuevo
e29 = as.numeric(e28b)
#2.10 Busca en la ayuda que hace la función scan(). Utilízala para leer un fichero cualquiera y asigna la lectura a
#un vector “z”.
#La función scan() permite leer datos y cargarlos en R.
help(scan)
write(c(1:10), file="data")
e210 = scan(file = "data")
#2.11 Crea un vector x con 100 elementos, selecciona de ese vector una muestra al azar de tamaño 5. Busca que hace la
#función sample().
e211a = c(1:100)
e211b = sample(e211a,5)
help(sample)
#2.12 Genera un vector de tipo con 100 números entre el 1 y el 4 de forma random. Para ello mira en la ayuda la
#función runif(). Obliga a que el vector resultante se ade tipo integer. Ordena el vector por tamaño usando la
#función sort(). ¿Qué devuelve la función sort?. SI quisieras invertir el orden de los elementos del vector que
#función utilizarías. Utiliza la función order() sobre x. ¿Cuál es la diferencia con la función sort()?
help(runif)
e212a = as.integer(runif(100,1,4))
e212b = sort(e212a)
help(sort)
#sort devuelve un vector ordeando ascendentemente o descendentemente.
e212c = sort(e212b, decreasing = TRUE)
e212d = order(e212c)
help(order)
#order devuelve los índices ordenados del vector
#Leemos el dataset subclus
subclus <- read.table("subclus.txt", sep=",")
setwd("~/Dropbox/zMaster/zRStudio/Master-en-Ciencia-De-Datos-e-Ingeniería-de-Computadores2/3 Mineria de Datos, Aspectos Avanzados/Clasificacion con conjuntos de datos no balanceados, no equilibrados")
setwd("~/Dropbox/zMaster/zRStudio/Master-en-Ciencia-De-Datos-e-Ingeniería-de-Computadores2/3 Mineria de Datos, Aspectos Avanzados/Clasificacion con conjuntos de datos no balanceados, no equilibrados")
#Implementación y evaluación de técnicas de clasificación imbalanceada
#Leemos el dataset subclus
subclus <- read.table("subclus.txt", sep=",")
colnames(subclus) <- c("Att1", "Att2", "Class")
unique(subclus$Class)
nClass0 <- sum(subclus$Class == 0)
nClass1 <- sum(subclus$Class == 1)
IR <- nClass1 / nClass0
IR #Cada ejemplo positivo hay 5 negativos
plot(subclus$Att1, subclus$Att2)
points(subclus[subclus$Class==0,1],subclus[subclus$Class==0,2],col="red")
points(subclus[subclus$Class==1,1],subclus[subclus$Class==1,2],col="blue")
pos <- (1:dim(subclus)[1])[subclus$Class==0]
neg <- (1:dim(subclus)[1])[subclus$Class==1]
CVperm_pos <- matrix(sample(pos,length(pos)), ncol=5, byrow=T)
CVperm_neg <- matrix(sample(neg,length(neg)), ncol=5, byrow=T)
CVperm <- rbind(CVperm_pos, CVperm_neg)
pos
neg
CVperm_pos
CVperm
library(class)
knn.pred = NULL
for( i in 1:5){
predictions <- knn(subclus[-CVperm[,i], -3], subclus[CVperm[,i], -3], subclus[-CVperm[,i], 3], k = 3)
knn.pred <- c(knn.pred, predictions)
}
acc <- sum((subclus$Class[as.vector(CVperm)] == 0 & knn.pred == 1)
| (subclus$Class[as.vector(CVperm)] == 1 & knn.pred == 2)) / (nClass0 + nClass1) #parece que se obitene un buen rendimiento
acc
acc
set.seed(1234)
library(class)
knn.pred = NULL
for( i in 1:5){
predictions <- knn(subclus[-CVperm[,i], -3], subclus[CVperm[,i], -3], subclus[-CVperm[,i], 3], k = 3)
knn.pred <- c(knn.pred, predictions)
}
acc <- sum((subclus$Class[as.vector(CVperm)] == 0 & knn.pred == 1)
library(class)
knn.pred = NULL
set.seed(1234)
for( i in 1:5){
predictions <- knn(subclus[-CVperm[,i], -3], subclus[CVperm[,i], -3], subclus[-CVperm[,i], 3], k = 3)
knn.pred <- c(knn.pred, predictions)
}
acc <- sum((subclus$Class[as.vector(CVperm)] == 0 & knn.pred == 1)
| (subclus$Class[as.vector(CVperm)] == 1 & knn.pred == 2)) / (nClass0 + nClass1)
acc
tpr <- sum(subclus$Class[as.vector(CVperm)] == 0 & knn.pred == 1) / nClass0
tpr
tnr <- sum(subclus$Class[as.vector(CVperm)] == 1 & knn.pred == 2) / nClass1
tnr
tnr <- sum(subclus$Class[as.vector(CVperm)] == 1 & knn.pred == 2) / nClass1
tnr
gmean <- sqrt(tpr * tnr) # no es tan bueno como decia el accuracy
gmean
knn.pred = NULL
for( i in 1:5){
train <- subclus[-CVperm[,i], -3]
classes.train <- subclus[-CVperm[,i], 3]
test  <- subclus[CVperm[,i], -3]
# randomly oversample the minority class (class 0)
minority.indices <- (1:dim(train)[1])[classes.train == 0]
to.add <- dim(train)[1] - 2 * length(minority.indices)
duplicate <- sample(minority.indices, to.add, replace = T)
for( j in 1:length(duplicate)){
train <- rbind(train, train[duplicate[j],])
classes.train <- c(classes.train, 0)
}
# use the modified training set to make predictions
predictions <-  knn(train, test, classes.train, k = 3)
knn.pred <- c(knn.pred, predictions)
} #se replican los ejemplos positivos
set.seed(1234)
for( i in 1:5){
train <- subclus[-CVperm[,i], -3]
classes.train <- subclus[-CVperm[,i], 3]
test  <- subclus[CVperm[,i], -3]
#Aleatoriamente se hace oversample de la clase minoritaria (class 0)
minority.indices <- (1:dim(train)[1])[classes.train == 0]
to.add <- dim(train)[1] - 2 * length(minority.indices)
duplicate <- sample(minority.indices, to.add, replace = T)
for( j in 1:length(duplicate)){
train <- rbind(train, train[duplicate[j],])
classes.train <- c(classes.train, 0)
}
# use the modified training set to make predictions
predictions <-  knn(train, test, classes.train, k = 3)
knn.pred <- c(knn.pred, predictions)
}
tpr.ROS <- sum(subclus$Class[as.vector(CVperm)] == 0 & knn.pred == 1) / nClass0 #ha aumentado
tpr.ROS
knn.pred = NULL
set.seed(1234)
for( i in 1:5){
train <- subclus[-CVperm[,i], -3]
classes.train <- subclus[-CVperm[,i], 3]
test  <- subclus[CVperm[,i], -3]
#Aleatoriamente se hace oversample de la clase minoritaria (class 0)
minority.indices <- (1:dim(train)[1])[classes.train == 0]
to.add <- dim(train)[1] - 2 * length(minority.indices)
duplicate <- sample(minority.indices, to.add, replace = T)
for( j in 1:length(duplicate)){
train <- rbind(train, train[duplicate[j],])
classes.train <- c(classes.train, 0)
}
# use the modified training set to make predictions
predictions <-  knn(train, test, classes.train, k = 3)
knn.pred <- c(knn.pred, predictions)
}
tpr.ROS <- sum(subclus$Class[as.vector(CVperm)] == 0 & knn.pred == 1) / nClass0 #ha aumentado
tpr.ROS
tnr.ROS <- sum(subclus$Class[as.vector(CVperm)] == 1 & knn.pred == 2) / nClass1 #ha bajado
tnr.ROS
gmean.ROS <- sqrt(tpr.ROS * tnr.ROS)
gmean.ROS
set.seed(1234)
knn.pred = NULL
for( i in 1:5){
train <- subclus[-CVperm[,i], -3]
classes.train <- subclus[-CVperm[,i], 3]
test  <- subclus[CVperm[,i], -3]
#Aleatoriamente aplicamos undersample a la clase minoritaria (class 1)
majority.indices <- (1:dim(train)[1])[classes.train == 1]
to.remove <- 2* length(majority.indices) - dim(train)[1]
remove <- sample(majority.indices, to.remove, replace = F)
train <- train[-remove,]
classes.train <- classes.train[-remove]
#Uso el training set modificado para hacer las predicciones
predictions <-  knn(train, test, classes.train, k = 3)
knn.pred <- c(knn.pred, predictions)
}
tpr.RUS <- sum(subclus$Class[as.vector(CVperm)] == 0 & knn.pred == 1) / nClass0
tpr.RUS
tnr.RUS <- sum(subclus$Class[as.vector(CVperm)] == 1 & knn.pred == 2) / nClass1
tnr.RUS
gmean.RUS <- sqrt(tpr.RUS * tnr.RUS)
gmean.RUS
subclus.RUS <- subclus
majority.indices <- (1:dim(subclus.RUS)[1])[subclus.RUS$Class == 1]
to.remove <- 2 * length(majority.indices) - dim(subclus.RUS)[1]
remove <- sample(majority.indices, to.remove, replace = F)
subclus.RUS <- subclus.RUS[-remove,]
plot(subclus.RUS$Att1, subclus.RUS$Att2)
points(subclus.RUS[subclus.RUS$Class==0,1],subclus.RUS[subclus.RUS$Class==0,2],col="red")
points(subclus.RUS[subclus.RUS$Class==1,1],subclus.RUS[subclus.RUS$Class==1,2],col="blue")
# 1.4.1 Función de distancia
distance <- function(i, j, data){
sum <- 0
for(f in 1:dim(data)[2]){
if(is.factor(data[,f])){ # nominal feature
if(data[i,f] != data[j,f]){
sum <- sum + 1
}
} else {
sum <- sum + (data[i,f] - data[j,f]) * (data[i,f] - data[j,f])
}
}
sum <- sqrt(sum)
return(sum)
}
